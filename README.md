# ğŸŒ¦ï¸ Weather Data Pipeline

This project is a modular and extendable data pipeline that fetches, cleans, and stores real-time weather data using the [Open-Meteo API](https://open-meteo.com/). It is designed with clean code practices and serves as a foundational project for learning **data engineering**.

---

## âœ… Features

- ğŸ“¡ Fetches hourly weather data using Open-Meteo's free API
- ğŸ§¼ Cleans and validates raw JSON response
- ğŸ’¾ Saves data locally in partitioned **Parquet** files (for efficient analytics)
- ğŸ“ Organized folder structure for scalability
- ğŸ¯ Modular design using layers: `ingest/`, `transform/`, `load/`
- ğŸ“‹ Logs to terminal and file using a centralized, color-coded logger

---

## ğŸ§± Project Structure

```

weather-pipeline/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ logger.py              # Centralized colorful logger
â”œâ”€â”€ ingest/
â”‚   â””â”€â”€ fetch\_weather.py       # API data fetching
â”œâ”€â”€ transform/
â”‚   â””â”€â”€ clean\_weather.py       # Data cleaning
â”œâ”€â”€ load/
â”‚   â””â”€â”€ save\_parquet.py        # Save to Parquet
â”œâ”€â”€ data/                      # Partitioned data files
â”œâ”€â”€ main.py                    # Entry point for the pipeline
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

````

---

## ğŸ“¦ How It Works

1. **Fetch Weather Data**  
   Calls the Open-Meteo API for a given location (default: New Delhi).

2. **Clean & Transform**  
   Parses and filters the raw response, converting it into a structured pandas DataFrame.

3. **Save as Parquet**  
   The cleaned data is saved to a local folder as `.parquet` files, partitioned by date.

---

## ğŸ“‹ How to Run

1. **Install dependencies**:

```bash
pip install -r requirements.txt
````

2. **Run the pipeline**:

```bash
python main.py
```

3. **Check output**:

   * Logs will appear in the terminal and `logs/pipeline.log`
   * Data will be saved in `data/YYYY-MM-DD/weather.parquet`

---

## ğŸ“Œ Dependencies

* `pandas`
* `requests`
* `pyarrow` (for Parquet support)
* `rich` (for colorful logging)

---

## ğŸ“ˆ What's Next?

* Add unit tests for each module
* Support multiple cities
* Load data into a PostgreSQL database
* Automate using Airflow or Prefect
* Add a simple dashboard using Streamlit or Superset

---

## ğŸ“š Learning Goals

This project helps you learn:

* API ingestion
* Data cleaning with pandas
* File formats for big data (Parquet)
* Project modularization
* Logging and error handling

---

## ğŸ“„ License

MIT License

---
